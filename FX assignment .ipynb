{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1eb879d",
   "metadata": {},
   "source": [
    "# Task 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719f713",
   "metadata": {},
   "source": [
    "To compare the new algorithm to a baseline algorithm, we give the baseline some default settings and get its accuracy on the dataset. Then, we change some settings 100 times and compare each result using 10-fold cross-validation with the baseline algorithm to get the best algorithm. \n",
    "\n",
    "Since the training and testing is only based on half of the dataset (not the full dataset), there will exist sampling error and might cause over-estimation of performance. This means that when the two models are evaluated on the second half of the data, we are not supposed to suspect that the new algorithm (with the best-performing hyper-parameter setting) outperforming the baseline (with the standard hyper-parameter setting). And the best model from the first half of the data might be outperformed by the baseline in the second half of the data.\n",
    "\n",
    "But, if most of the models can outperform the baseline on the first half, it is more likely to expect the result will be the same in the second half, that is, the best model will still outperform the baseline.\n",
    "\n",
    "The code is as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b27b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy (baseline): 0.908\n",
      "Train set accuracy (best model): 0.912\n",
      "Test set accuracy (baseline): 0.932\n",
      "Test set accuracy (best model): 0.920\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=5)\n",
    "\n",
    "# Baseline model\n",
    "baseline = RandomForestClassifier(random_state=5)\n",
    "baseline_acc = cross_val_score(baseline, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "\n",
    "# Find best model\n",
    "best_model = None\n",
    "best_acc = 0\n",
    "\n",
    "for i in range(100):\n",
    "    n_estimators = np.random.randint(20, 80)\n",
    "    max_depth = np.random.randint(2, 8)\n",
    "    max_features = np.random.randint(2, 8)\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, random_state=42)\n",
    "    acc = cross_val_score(rfc, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_model = rfc\n",
    "        best_acc = acc\n",
    "\n",
    "# Evaluate best model and baseline on test set\n",
    "best_model.fit(X_train, y_train)\n",
    "best_acc_test = accuracy_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_acc_test = accuracy_score(y_test, baseline.predict(X_test))\n",
    "\n",
    "print(f\"Train set accuracy (baseline): {baseline_acc:.3f}\")\n",
    "print(f\"Train set accuracy (best model): {best_acc:.3f}\")\n",
    "print(f\"Test set accuracy (baseline): {baseline_acc_test:.3f}\")\n",
    "print(f\"Test set accuracy (best model): {best_acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b8e37",
   "metadata": {},
   "source": [
    "From the result, we can see that in the first half of the dataset, the best model has higher accuracy than the baseline. But then in the second half of the dataset, the baseline has higher accuracy than the best model instead, as I said before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3aac4",
   "metadata": {},
   "source": [
    "# Task 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5a630",
   "metadata": {},
   "source": [
    "In this task we need to choose a binary classification model randomly (without training) on the dataset, and have collect the accuracy and AUC, and also observed a much higher precision for the majority class than for the minority class. \n",
    "\n",
    "Next, we use all the instances from the minority class and sampling (without replacement) 1000 instances from the majority class to form the new dataset and use the same binary classification model on it.\n",
    "\n",
    "1.For accuracy, I think it will drop in the new dataset. Because in the first situation, most of accuracy comes from the majority class, and if we build a new dataset, it just like decreasing the majority class and increasing the minority class. So, the proportion of misclassified instances will also increase, which casue the accuracy to be less.\n",
    "\n",
    "2.For the AUC, we do not expect it will change a lot because the AUC is not affected by the class proportions. But due to our new form of dataset and changed samples, AUC in our case may change slightly.\n",
    "\n",
    "3.Last, for the precision of majority and minority class, it might increase for the minority and decrease for the majority. Because by constructing the new dataset, some majority samples which have been predicted to be minority by mistake are moved out, and thus the precision of minority class will increase and for majority will decrease instead.\n",
    "\n",
    "The code is as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c9674d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the original dataset:\n",
      "Accuracy: 0.808\n",
      "AUC: 0.776\n",
      "Precision for majority class: 0.825\n",
      "Precision for minority class: 0.572\n",
      "\n",
      "Results on the new dataset:\n",
      "Accuracy: 0.575\n",
      "AUC: 0.772\n",
      "Precision for majority class: 0.541\n",
      "Precision for minority class: 0.818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# create the original dataset\n",
    "X, y = make_classification(n_samples=5000, n_features=10, n_informative=5, n_redundant=5, n_classes=2,\n",
    "                           weights=[0.8, 0.2], class_sep=1.0, random_state=48)\n",
    "\n",
    "# create the model and fit it on the original dataset\n",
    "model = LogisticRegression(random_state=48)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# calculate the accuracy, AUC, and precision for the original dataset\n",
    "acc = accuracy_score(y, y_pred)\n",
    "auc = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "prec_maj = precision_score(y, y_pred, pos_label=0)\n",
    "prec_min = precision_score(y, y_pred, pos_label=1)\n",
    "\n",
    "print(\"Results on the original dataset:\")\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"AUC: {auc:.3f}\")\n",
    "print(f\"Precision for majority class: {prec_maj:.3f}\")\n",
    "print(f\"Precision for minority class: {prec_min:.3f}\")\n",
    "\n",
    "# create the new dataset, use the same model to predict \n",
    "# and calculate the accuracy, AUC, and precision on it\n",
    "maj_indices = np.random.choice(np.where(y == 0)[0], size=1000, replace=False)\n",
    "X_new = np.vstack([X[y == 1], X[maj_indices]])\n",
    "y_new = np.concatenate([y[y == 1], y[maj_indices]])\n",
    "\n",
    "y_pred_new = model.predict(X_new)\n",
    "\n",
    "acc_new = accuracy_score(y_new, y_pred_new)\n",
    "auc_new = roc_auc_score(y_new, model.predict_proba(X_new)[:, 1])\n",
    "prec_maj_new = precision_score(y_new, y_pred_new, pos_label=0)\n",
    "prec_min_new = precision_score(y_new, y_pred_new, pos_label=1)\n",
    "\n",
    "print(\"\\nResults on the new dataset:\")\n",
    "print(f\"Accuracy: {acc_new:.3f}\")\n",
    "print(f\"AUC: {auc_new:.3f}\")\n",
    "print(f\"Precision for majority class: {prec_maj_new:.3f}\")\n",
    "print(f\"Precision for minority class: {prec_min_new:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1a66f",
   "metadata": {},
   "source": [
    "From the result, we can see that the accuracy really drops in the new dataset, and the AUC of these two are nearly the same. As for precision, in the original dataset, the precision for majority class is much higher than that of the minority class, and it decrease for majority and increase for minority in the new dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
